{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[I 2024-08-30 08:41:28,084] A new study created in memory with name: no-name-3cf29b14-5701-4abf-8987-5b3aa2359c10\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python310\\site-packages\\optuna\\distributions.py:700: UserWarning: The distribution is specified by [2, 39] and step=3, but the range is not divisible by `step`. It will be replaced by [2, 38].\n",
      "  warnings.warn(\n",
      "[I 2024-08-30 08:42:03,660] Trial 0 finished with value: 9.362822339653576e-08 and parameters: {'n_layers': 7, 'n_units_l0': 26, 'dropout_l0': 0.3969959614414517, 'n_units_l1': 20, 'dropout_l1': 0.4745989817613934, 'n_units_l2': 29, 'dropout_l2': 0.3856697277251102, 'n_units_l3': 20, 'dropout_l3': 0.20824421010224203, 'n_units_l4': 38, 'dropout_l4': 0.264057202164936, 'n_units_l5': 32, 'dropout_l5': 0.4431959980489172, 'n_units_l6': 26, 'dropout_l6': 0.4017543551431879, 'optimizer': 'rmsprop', 'learning_rate': 0.001492062009613993, 'batch_size': 256}. Best is trial 0 with value: 9.362822339653576e-08.\n",
      "[I 2024-08-30 08:42:29,652] Trial 1 finished with value: 1.7973030708162696e-07 and parameters: {'n_layers': 5, 'n_units_l0': 2, 'dropout_l0': 0.3385519677044167, 'n_units_l1': 26, 'dropout_l1': 0.4147757272922145, 'n_units_l2': 20, 'dropout_l2': 0.4339597013346137, 'n_units_l3': 8, 'dropout_l3': 0.49045511310342566, 'n_units_l4': 35, 'dropout_l4': 0.5788102181844939, 'optimizer': 'rmsprop', 'learning_rate': 0.004628338331217437, 'batch_size': 224}. Best is trial 0 with value: 9.362822339653576e-08.\n",
      "[I 2024-08-30 08:43:21,257] Trial 2 finished with value: 0.0024984017945826054 and parameters: {'n_layers': 6, 'n_units_l0': 17, 'dropout_l0': 0.3719476586830112, 'n_units_l1': 11, 'dropout_l1': 0.35845878278573495, 'n_units_l2': 35, 'dropout_l2': 0.424213958849482, 'n_units_l3': 2, 'dropout_l3': 0.33608985489656307, 'n_units_l4': 29, 'dropout_l4': 0.23510391702698336, 'n_units_l5': 35, 'dropout_l5': 0.35893419048751574, 'optimizer': 'rmsprop', 'learning_rate': 0.002854937145939216, 'batch_size': 32}. Best is trial 0 with value: 9.362822339653576e-08.\n",
      "[I 2024-08-30 08:43:33,275] Trial 3 finished with value: 1.0145446062088013 and parameters: {'n_layers': 6, 'n_units_l0': 20, 'dropout_l0': 0.5710208610702848, 'n_units_l1': 8, 'dropout_l1': 0.4842908652116275, 'n_units_l2': 23, 'dropout_l2': 0.4087815061899339, 'n_units_l3': 23, 'dropout_l3': 0.3744266451247404, 'n_units_l4': 14, 'dropout_l4': 0.5991457423859454, 'n_units_l5': 38, 'dropout_l5': 0.2697711580528009, 'optimizer': 'adam', 'learning_rate': 0.0001419365376598306, 'batch_size': 192}. Best is trial 0 with value: 9.362822339653576e-08.\n",
      "[I 2024-08-30 08:43:43,427] Trial 4 finished with value: 0.9508786797523499 and parameters: {'n_layers': 7, 'n_units_l0': 11, 'dropout_l0': 0.30398883931722664, 'n_units_l1': 8, 'dropout_l1': 0.3915562000849683, 'n_units_l2': 17, 'dropout_l2': 0.3229948726604383, 'n_units_l3': 11, 'dropout_l3': 0.5914550016469888, 'n_units_l4': 38, 'dropout_l4': 0.22751808792577874, 'n_units_l5': 17, 'dropout_l5': 0.4469593508164508, 'n_units_l6': 5, 'dropout_l6': 0.3313506201864437, 'optimizer': 'rmsprop', 'learning_rate': 0.00024819193071398205, 'batch_size': 160}. Best is trial 0 with value: 9.362822339653576e-08.\n",
      "[I 2024-08-30 08:43:48,307] Trial 5 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 08:43:52,667] Trial 6 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 08:43:57,193] Trial 7 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 08:44:00,885] Trial 8 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 08:44:04,164] Trial 9 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 08:44:09,541] Trial 10 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 08:44:12,897] Trial 11 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 08:44:25,458] Trial 12 finished with value: 1.1846836400764005e-07 and parameters: {'n_layers': 5, 'n_units_l0': 26, 'dropout_l0': 0.3801706174625356, 'n_units_l1': 23, 'dropout_l1': 0.48031429596888264, 'n_units_l2': 29, 'dropout_l2': 0.3363089083899238, 'n_units_l3': 2, 'dropout_l3': 0.3072502340513906, 'n_units_l4': 32, 'dropout_l4': 0.45156461957153665, 'optimizer': 'rmsprop', 'learning_rate': 0.004758702030433512, 'batch_size': 128}. Best is trial 0 with value: 9.362822339653576e-08.\n",
      "[I 2024-08-30 08:44:28,462] Trial 13 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 08:44:54,665] Trial 14 finished with value: 3.948755420424277e-06 and parameters: {'n_layers': 5, 'n_units_l0': 14, 'dropout_l0': 0.4259729764592989, 'n_units_l1': 38, 'dropout_l1': 0.5590208349180754, 'n_units_l2': 29, 'dropout_l2': 0.25942942394128043, 'n_units_l3': 20, 'dropout_l3': 0.2836242814583518, 'n_units_l4': 26, 'dropout_l4': 0.4538883888063007, 'optimizer': 'adam', 'learning_rate': 0.008781565037293488, 'batch_size': 128}. Best is trial 0 with value: 9.362822339653576e-08.\n",
      "[I 2024-08-30 08:44:58,520] Trial 15 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 08:45:02,566] Trial 16 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 08:45:18,388] Trial 17 finished with value: 0.0 and parameters: {'n_layers': 5, 'n_units_l0': 20, 'dropout_l0': 0.4820348239796654, 'n_units_l1': 26, 'dropout_l1': 0.44931243491052075, 'n_units_l2': 8, 'dropout_l2': 0.486763440295845, 'n_units_l3': 38, 'dropout_l3': 0.31672904084354947, 'n_units_l4': 5, 'dropout_l4': 0.40487287879337097, 'optimizer': 'rmsprop', 'learning_rate': 0.004644319429751065, 'batch_size': 128}. Best is trial 17 with value: 0.0.\n",
      "[I 2024-08-30 08:45:24,129] Trial 18 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 08:45:27,918] Trial 19 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 08:45:31,537] Trial 20 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 08:45:51,692] Trial 21 finished with value: 0.0 and parameters: {'n_layers': 5, 'n_units_l0': 29, 'dropout_l0': 0.40534011276043014, 'n_units_l1': 23, 'dropout_l1': 0.4446512016236048, 'n_units_l2': 26, 'dropout_l2': 0.35921530504599114, 'n_units_l3': 14, 'dropout_l3': 0.3215689638806156, 'n_units_l4': 23, 'dropout_l4': 0.4302639945008912, 'optimizer': 'rmsprop', 'learning_rate': 0.005658728016362482, 'batch_size': 128}. Best is trial 17 with value: 0.0.\n",
      "[I 2024-08-30 08:46:20,236] Trial 22 finished with value: 2.339730631462089e-10 and parameters: {'n_layers': 5, 'n_units_l0': 29, 'dropout_l0': 0.42161056680374753, 'n_units_l1': 23, 'dropout_l1': 0.5194025862593569, 'n_units_l2': 23, 'dropout_l2': 0.45645105072617653, 'n_units_l3': 17, 'dropout_l3': 0.38089332023215167, 'n_units_l4': 23, 'dropout_l4': 0.424883020082172, 'optimizer': 'rmsprop', 'learning_rate': 0.005844830149052141, 'batch_size': 128}. Best is trial 17 with value: 0.0.\n",
      "[I 2024-08-30 08:46:48,969] Trial 23 finished with value: 0.0 and parameters: {'n_layers': 5, 'n_units_l0': 32, 'dropout_l0': 0.422562213470872, 'n_units_l1': 29, 'dropout_l1': 0.4329826905466936, 'n_units_l2': 23, 'dropout_l2': 0.4506400371414512, 'n_units_l3': 14, 'dropout_l3': 0.389744033674968, 'n_units_l4': 23, 'dropout_l4': 0.42156428680570757, 'optimizer': 'rmsprop', 'learning_rate': 0.0064849597155912375, 'batch_size': 128}. Best is trial 17 with value: 0.0.\n",
      "[I 2024-08-30 08:46:55,542] Trial 24 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 08:47:26,749] Trial 25 pruned. Trial was pruned at epoch 25.\n",
      "[I 2024-08-30 08:48:09,416] Trial 26 finished with value: 0.0 and parameters: {'n_layers': 5, 'n_units_l0': 23, 'dropout_l0': 0.28130361755344013, 'n_units_l1': 35, 'dropout_l1': 0.34468738928730275, 'n_units_l2': 17, 'dropout_l2': 0.27958752589816294, 'n_units_l3': 14, 'dropout_l3': 0.4469328777370083, 'n_units_l4': 17, 'dropout_l4': 0.3698968599654204, 'optimizer': 'adam', 'learning_rate': 0.003356079927846326, 'batch_size': 128}. Best is trial 17 with value: 0.0.\n",
      "[I 2024-08-30 08:49:00,865] Trial 27 finished with value: 0.0 and parameters: {'n_layers': 5, 'n_units_l0': 35, 'dropout_l0': 0.46029887642000866, 'n_units_l1': 23, 'dropout_l1': 0.4326676862592289, 'n_units_l2': 26, 'dropout_l2': 0.3553268950565817, 'n_units_l3': 14, 'dropout_l3': 0.3106114900895369, 'n_units_l4': 26, 'dropout_l4': 0.425733079334241, 'optimizer': 'rmsprop', 'learning_rate': 0.006674982378669761, 'batch_size': 96}. Best is trial 17 with value: 0.0.\n",
      "[I 2024-08-30 08:49:07,706] Trial 28 pruned. Trial was pruned at epoch 1.\n",
      "[I 2024-08-30 08:49:13,053] Trial 29 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 08:49:18,601] Trial 30 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 08:49:52,085] Trial 31 finished with value: 0.0 and parameters: {'n_layers': 5, 'n_units_l0': 23, 'dropout_l0': 0.2784887899087726, 'n_units_l1': 35, 'dropout_l1': 0.3536177922766479, 'n_units_l2': 17, 'dropout_l2': 0.2690695422821392, 'n_units_l3': 14, 'dropout_l3': 0.4489824881128227, 'n_units_l4': 17, 'dropout_l4': 0.3546378345305686, 'optimizer': 'adam', 'learning_rate': 0.006870296825357057, 'batch_size': 128}. Best is trial 17 with value: 0.0.\n",
      "[I 2024-08-30 08:50:00,580] Trial 32 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 08:50:08,108] Trial 33 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 08:50:15,894] Trial 34 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 08:50:25,453] Trial 35 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 08:50:41,228] Trial 36 pruned. Trial was pruned at epoch 10.\n",
      "[I 2024-08-30 08:50:47,012] Trial 37 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 08:50:53,934] Trial 38 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 08:51:02,857] Trial 39 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 08:51:09,025] Trial 40 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 08:51:49,013] Trial 41 finished with value: 9.468109851695772e-08 and parameters: {'n_layers': 5, 'n_units_l0': 35, 'dropout_l0': 0.46075602218673506, 'n_units_l1': 23, 'dropout_l1': 0.44625617195710904, 'n_units_l2': 26, 'dropout_l2': 0.3560251791837233, 'n_units_l3': 14, 'dropout_l3': 0.3081611892110182, 'n_units_l4': 26, 'dropout_l4': 0.425024863789273, 'optimizer': 'rmsprop', 'learning_rate': 0.006436183123035809, 'batch_size': 96}. Best is trial 17 with value: 0.0.\n",
      "[I 2024-08-30 08:52:02,009] Trial 42 pruned. Trial was pruned at epoch 8.\n",
      "[I 2024-08-30 08:52:24,076] Trial 43 finished with value: 0.0 and parameters: {'n_layers': 5, 'n_units_l0': 35, 'dropout_l0': 0.4002201068642623, 'n_units_l1': 23, 'dropout_l1': 0.396425332392428, 'n_units_l2': 23, 'dropout_l2': 0.5066120805998415, 'n_units_l3': 5, 'dropout_l3': 0.3501233216150507, 'n_units_l4': 23, 'dropout_l4': 0.4863792726624006, 'optimizer': 'rmsprop', 'learning_rate': 0.008237998208256096, 'batch_size': 128}. Best is trial 17 with value: 0.0.\n",
      "[I 2024-08-30 08:52:53,887] Trial 44 finished with value: 5.849327133766735e-10 and parameters: {'n_layers': 5, 'n_units_l0': 32, 'dropout_l0': 0.4991151411257711, 'n_units_l1': 14, 'dropout_l1': 0.4819566798487538, 'n_units_l2': 17, 'dropout_l2': 0.31095828538173304, 'n_units_l3': 11, 'dropout_l3': 0.456092088660787, 'n_units_l4': 29, 'dropout_l4': 0.3555330012122716, 'optimizer': 'rmsprop', 'learning_rate': 0.004416278625191665, 'batch_size': 96}. Best is trial 17 with value: 0.0.\n",
      "[I 2024-08-30 08:53:20,257] Trial 45 finished with value: 0.0 and parameters: {'n_layers': 5, 'n_units_l0': 29, 'dropout_l0': 0.45989023710017946, 'n_units_l1': 29, 'dropout_l1': 0.3603500047655544, 'n_units_l2': 32, 'dropout_l2': 0.29225198134800445, 'n_units_l3': 17, 'dropout_l3': 0.32777075718230964, 'n_units_l4': 35, 'dropout_l4': 0.43409115376599944, 'optimizer': 'rmsprop', 'learning_rate': 0.006735876860234507, 'batch_size': 128}. Best is trial 17 with value: 0.0.\n",
      "[I 2024-08-30 08:53:26,568] Trial 46 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 08:53:43,617] Trial 47 pruned. Trial was pruned at epoch 9.\n",
      "[I 2024-08-30 08:53:51,230] Trial 48 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 08:53:59,602] Trial 49 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 08:54:05,879] Trial 50 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 08:54:17,063] Trial 51 pruned. Trial was pruned at epoch 5.\n",
      "[I 2024-08-30 08:54:34,026] Trial 52 pruned. Trial was pruned at epoch 10.\n",
      "[I 2024-08-30 08:54:41,917] Trial 53 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 08:54:54,782] Trial 54 pruned. Trial was pruned at epoch 6.\n",
      "[I 2024-08-30 08:55:02,374] Trial 55 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 08:55:34,748] Trial 56 finished with value: 0.0 and parameters: {'n_layers': 5, 'n_units_l0': 29, 'dropout_l0': 0.23096122546989722, 'n_units_l1': 38, 'dropout_l1': 0.4725856221277963, 'n_units_l2': 29, 'dropout_l2': 0.4937570067007361, 'n_units_l3': 20, 'dropout_l3': 0.5119784773728663, 'n_units_l4': 26, 'dropout_l4': 0.4108155312864804, 'optimizer': 'adam', 'learning_rate': 0.009726124088165228, 'batch_size': 128}. Best is trial 17 with value: 0.0.\n",
      "[I 2024-08-30 08:55:40,709] Trial 57 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 08:55:46,544] Trial 58 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 08:55:53,586] Trial 59 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 08:55:59,917] Trial 60 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 08:56:20,696] Trial 61 finished with value: 0.0 and parameters: {'n_layers': 5, 'n_units_l0': 35, 'dropout_l0': 0.40030048556936093, 'n_units_l1': 23, 'dropout_l1': 0.3988291724968422, 'n_units_l2': 23, 'dropout_l2': 0.5160885441012766, 'n_units_l3': 5, 'dropout_l3': 0.3539278345298777, 'n_units_l4': 23, 'dropout_l4': 0.4597266306569552, 'optimizer': 'rmsprop', 'learning_rate': 0.008482090778533865, 'batch_size': 128}. Best is trial 17 with value: 0.0.\n",
      "[I 2024-08-30 08:56:26,932] Trial 62 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 08:56:32,935] Trial 63 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 08:57:11,083] Trial 64 finished with value: 0.0 and parameters: {'n_layers': 5, 'n_units_l0': 35, 'dropout_l0': 0.5113099314044175, 'n_units_l1': 29, 'dropout_l1': 0.3462225173228759, 'n_units_l2': 23, 'dropout_l2': 0.46882982969340786, 'n_units_l3': 35, 'dropout_l3': 0.33957524849931997, 'n_units_l4': 17, 'dropout_l4': 0.39419291353335606, 'optimizer': 'rmsprop', 'learning_rate': 0.006834582923011825, 'batch_size': 96}. Best is trial 17 with value: 0.0.\n",
      "[I 2024-08-30 08:57:16,555] Trial 65 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 08:57:27,041] Trial 66 pruned. Trial was pruned at epoch 5.\n",
      "[I 2024-08-30 08:57:33,096] Trial 67 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 08:57:40,283] Trial 68 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 08:57:45,945] Trial 69 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 08:57:53,364] Trial 70 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 08:58:00,775] Trial 71 pruned. Trial was pruned at epoch 2.\n",
      "[I 2024-08-30 08:58:32,074] Trial 72 finished with value: 1.1698653157310446e-10 and parameters: {'n_layers': 5, 'n_units_l0': 32, 'dropout_l0': 0.4341306820914566, 'n_units_l1': 35, 'dropout_l1': 0.36191480870304127, 'n_units_l2': 32, 'dropout_l2': 0.26058610438451696, 'n_units_l3': 17, 'dropout_l3': 0.3280861674369628, 'n_units_l4': 32, 'dropout_l4': 0.45800554523360004, 'optimizer': 'rmsprop', 'learning_rate': 0.007022292819402406, 'batch_size': 160}. Best is trial 17 with value: 0.0.\n",
      "[I 2024-08-30 08:58:38,079] Trial 73 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 08:58:58,634] Trial 74 finished with value: 6.62923724803477e-08 and parameters: {'n_layers': 5, 'n_units_l0': 26, 'dropout_l0': 0.46771358487097514, 'n_units_l1': 32, 'dropout_l1': 0.3572014298210112, 'n_units_l2': 35, 'dropout_l2': 0.31649657936612013, 'n_units_l3': 20, 'dropout_l3': 0.30093932471082857, 'n_units_l4': 35, 'dropout_l4': 0.38254965859201195, 'optimizer': 'rmsprop', 'learning_rate': 0.009870772888876972, 'batch_size': 128}. Best is trial 17 with value: 0.0.\n",
      "[I 2024-08-30 08:59:04,484] Trial 75 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 08:59:10,475] Trial 76 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 08:59:26,358] Trial 77 pruned. Trial was pruned at epoch 23.\n",
      "[I 2024-08-30 08:59:33,987] Trial 78 pruned. Trial was pruned at epoch 3.\n",
      "[I 2024-08-30 08:59:44,145] Trial 79 pruned. Trial was pruned at epoch 1.\n",
      "[I 2024-08-30 08:59:50,074] Trial 80 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 08:59:59,685] Trial 81 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 09:00:25,857] Trial 82 pruned. Trial was pruned at epoch 25.\n",
      "[I 2024-08-30 09:00:43,600] Trial 83 pruned. Trial was pruned at epoch 14.\n",
      "[I 2024-08-30 09:00:51,132] Trial 84 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 09:00:58,827] Trial 85 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 09:01:06,045] Trial 86 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 09:01:27,093] Trial 87 finished with value: 0.0 and parameters: {'n_layers': 5, 'n_units_l0': 26, 'dropout_l0': 0.3822299356350624, 'n_units_l1': 35, 'dropout_l1': 0.3557087041962927, 'n_units_l2': 26, 'dropout_l2': 0.2655315037727582, 'n_units_l3': 14, 'dropout_l3': 0.36077600630314405, 'n_units_l4': 26, 'dropout_l4': 0.4318414572845495, 'optimizer': 'rmsprop', 'learning_rate': 0.007213160195402871, 'batch_size': 128}. Best is trial 17 with value: 0.0.\n",
      "[I 2024-08-30 09:01:34,330] Trial 88 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 09:01:39,737] Trial 89 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 09:01:46,538] Trial 90 pruned. Trial was pruned at epoch 2.\n",
      "[I 2024-08-30 09:01:52,126] Trial 91 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 09:02:57,345] Trial 92 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 09:02:59,652] Trial 93 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 09:03:19,229] Trial 94 finished with value: 3.1196409344680376e-10 and parameters: {'n_layers': 5, 'n_units_l0': 35, 'dropout_l0': 0.2866606007846077, 'n_units_l1': 26, 'dropout_l1': 0.4225752772712806, 'n_units_l2': 20, 'dropout_l2': 0.4740307687612818, 'n_units_l3': 11, 'dropout_l3': 0.33747212314685177, 'n_units_l4': 23, 'dropout_l4': 0.43583621326571026, 'optimizer': 'rmsprop', 'learning_rate': 0.007812599714032194, 'batch_size': 128}. Best is trial 17 with value: 0.0.\n",
      "[I 2024-08-30 09:03:24,766] Trial 95 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 09:03:30,154] Trial 96 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 09:03:40,664] Trial 97 pruned. Trial was pruned at epoch 4.\n",
      "[I 2024-08-30 09:03:46,173] Trial 98 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 09:03:51,822] Trial 99 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 09:03:58,927] Trial 100 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 09:04:10,796] Trial 101 pruned. Trial was pruned at epoch 3.\n",
      "[I 2024-08-30 09:04:17,533] Trial 102 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 09:04:43,816] Trial 103 finished with value: 0.0 and parameters: {'n_layers': 5, 'n_units_l0': 32, 'dropout_l0': 0.4517770848764895, 'n_units_l1': 29, 'dropout_l1': 0.3335394323046199, 'n_units_l2': 26, 'dropout_l2': 0.38365894678881995, 'n_units_l3': 38, 'dropout_l3': 0.31382385335215573, 'n_units_l4': 20, 'dropout_l4': 0.4852861657572778, 'optimizer': 'rmsprop', 'learning_rate': 0.006585605940404403, 'batch_size': 128}. Best is trial 17 with value: 0.0.\n",
      "[I 2024-08-30 09:04:52,577] Trial 104 pruned. Trial was pruned at epoch 1.\n",
      "[I 2024-08-30 09:04:58,471] Trial 105 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 09:05:04,920] Trial 106 pruned. Trial was pruned at epoch 1.\n",
      "[I 2024-08-30 09:05:10,412] Trial 107 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 09:05:18,216] Trial 108 pruned. Trial was pruned at epoch 1.\n",
      "[I 2024-08-30 09:05:30,540] Trial 109 pruned. Trial was pruned at epoch 10.\n",
      "[I 2024-08-30 09:05:59,509] Trial 110 finished with value: 0.0 and parameters: {'n_layers': 5, 'n_units_l0': 26, 'dropout_l0': 0.38135575799713706, 'n_units_l1': 23, 'dropout_l1': 0.36969963644895465, 'n_units_l2': 23, 'dropout_l2': 0.48637609302138074, 'n_units_l3': 23, 'dropout_l3': 0.3511098371273099, 'n_units_l4': 26, 'dropout_l4': 0.3631009529564759, 'optimizer': 'rmsprop', 'learning_rate': 0.005989204347194865, 'batch_size': 128}. Best is trial 17 with value: 0.0.\n",
      "[I 2024-08-30 09:06:23,448] Trial 111 finished with value: 3.041649820900716e-09 and parameters: {'n_layers': 5, 'n_units_l0': 26, 'dropout_l0': 0.302824490372832, 'n_units_l1': 35, 'dropout_l1': 0.3544035639117182, 'n_units_l2': 26, 'dropout_l2': 0.2587557510857776, 'n_units_l3': 14, 'dropout_l3': 0.3629837903259093, 'n_units_l4': 26, 'dropout_l4': 0.4362603161575756, 'optimizer': 'rmsprop', 'learning_rate': 0.007407845560045675, 'batch_size': 128}. Best is trial 17 with value: 0.0.\n",
      "[I 2024-08-30 09:06:29,615] Trial 112 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 09:06:35,657] Trial 113 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 09:06:42,898] Trial 114 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 09:06:48,808] Trial 115 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 09:06:56,635] Trial 116 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 09:07:05,629] Trial 117 pruned. Trial was pruned at epoch 1.\n",
      "[I 2024-08-30 09:07:11,269] Trial 118 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 09:07:20,997] Trial 119 pruned. Trial was pruned at epoch 4.\n",
      "[I 2024-08-30 09:17:08,712] Trial 120 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 09:17:10,872] Trial 121 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 09:17:12,321] Trial 122 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 09:17:13,811] Trial 123 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 09:17:16,295] Trial 124 pruned. Trial was pruned at epoch 1.\n",
      "[I 2024-08-30 09:17:17,992] Trial 125 pruned. Trial was pruned at epoch 1.\n",
      "[I 2024-08-30 09:17:27,380] Trial 126 finished with value: 1.5598204672340188e-10 and parameters: {'n_layers': 5, 'n_units_l0': 29, 'dropout_l0': 0.4068182062251066, 'n_units_l1': 29, 'dropout_l1': 0.3351772274590851, 'n_units_l2': 23, 'dropout_l2': 0.40278414704157917, 'n_units_l3': 14, 'dropout_l3': 0.28528833399990905, 'n_units_l4': 20, 'dropout_l4': 0.4883880590422324, 'optimizer': 'rmsprop', 'learning_rate': 0.007845975072836438, 'batch_size': 128}. Best is trial 17 with value: 0.0.\n",
      "[I 2024-08-30 09:17:29,251] Trial 127 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 09:17:31,422] Trial 128 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 09:17:34,297] Trial 129 pruned. Trial was pruned at epoch 3.\n",
      "[I 2024-08-30 09:17:36,498] Trial 130 pruned. Trial was pruned at epoch 1.\n",
      "[I 2024-08-30 09:17:41,098] Trial 131 pruned. Trial was pruned at epoch 13.\n",
      "[I 2024-08-30 09:17:51,614] Trial 132 finished with value: 7.604124441229487e-09 and parameters: {'n_layers': 5, 'n_units_l0': 26, 'dropout_l0': 0.3397685715448082, 'n_units_l1': 23, 'dropout_l1': 0.3950731193385674, 'n_units_l2': 23, 'dropout_l2': 0.4833032581293253, 'n_units_l3': 23, 'dropout_l3': 0.3282369595691232, 'n_units_l4': 26, 'dropout_l4': 0.3266636251115383, 'optimizer': 'rmsprop', 'learning_rate': 0.005997385614445553, 'batch_size': 128}. Best is trial 17 with value: 0.0.\n",
      "[I 2024-08-30 09:17:54,943] Trial 133 pruned. Trial was pruned at epoch 9.\n",
      "[I 2024-08-30 09:17:56,578] Trial 134 pruned. Trial was pruned at epoch 1.\n",
      "[I 2024-08-30 09:17:58,023] Trial 135 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 09:18:00,933] Trial 136 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 09:18:02,694] Trial 137 pruned. Trial was pruned at epoch 1.\n",
      "[I 2024-08-30 09:18:04,126] Trial 138 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 09:18:07,332] Trial 139 pruned. Trial was pruned at epoch 1.\n",
      "[I 2024-08-30 09:18:18,066] Trial 140 finished with value: 0.0 and parameters: {'n_layers': 5, 'n_units_l0': 29, 'dropout_l0': 0.40794030003885723, 'n_units_l1': 32, 'dropout_l1': 0.3565363804410062, 'n_units_l2': 14, 'dropout_l2': 0.3153836146375759, 'n_units_l3': 11, 'dropout_l3': 0.4352890444228582, 'n_units_l4': 29, 'dropout_l4': 0.3720685276786337, 'optimizer': 'rmsprop', 'learning_rate': 0.007650279955160729, 'batch_size': 128}. Best is trial 17 with value: 0.0.\n",
      "[I 2024-08-30 09:18:25,804] Trial 141 finished with value: 0.0 and parameters: {'n_layers': 5, 'n_units_l0': 29, 'dropout_l0': 0.41344369018686755, 'n_units_l1': 32, 'dropout_l1': 0.3574396854885757, 'n_units_l2': 14, 'dropout_l2': 0.31499557547683366, 'n_units_l3': 11, 'dropout_l3': 0.4334453569888877, 'n_units_l4': 29, 'dropout_l4': 0.37354814946366094, 'optimizer': 'rmsprop', 'learning_rate': 0.007631794146224668, 'batch_size': 128}. Best is trial 17 with value: 0.0.\n",
      "[I 2024-08-30 09:18:27,309] Trial 142 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 09:18:28,760] Trial 143 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 09:18:49,415] Trial 144 finished with value: 1.6768070265626989e-09 and parameters: {'n_layers': 5, 'n_units_l0': 32, 'dropout_l0': 0.4438970984753536, 'n_units_l1': 38, 'dropout_l1': 0.34602828036239863, 'n_units_l2': 11, 'dropout_l2': 0.29181633870633655, 'n_units_l3': 32, 'dropout_l3': 0.41992224532975203, 'n_units_l4': 26, 'dropout_l4': 0.3916784287788881, 'optimizer': 'rmsprop', 'learning_rate': 0.005909583485027919, 'batch_size': 128}. Best is trial 17 with value: 0.0.\n",
      "[I 2024-08-30 09:18:55,421] Trial 145 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 09:19:01,522] Trial 146 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 09:19:26,920] Trial 147 finished with value: 1.9279380580883299e-07 and parameters: {'n_layers': 5, 'n_units_l0': 35, 'dropout_l0': 0.416763995390525, 'n_units_l1': 32, 'dropout_l1': 0.3534685199398154, 'n_units_l2': 17, 'dropout_l2': 0.2671548789076341, 'n_units_l3': 20, 'dropout_l3': 0.4386539752699214, 'n_units_l4': 29, 'dropout_l4': 0.40246815100130134, 'optimizer': 'rmsprop', 'learning_rate': 0.008598273339133064, 'batch_size': 96}. Best is trial 17 with value: 0.0.\n",
      "[I 2024-08-30 09:19:34,307] Trial 148 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 09:19:39,789] Trial 149 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 09:19:45,823] Trial 150 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 09:19:51,976] Trial 151 pruned. Trial was pruned at epoch 1.\n",
      "[I 2024-08-30 09:19:58,234] Trial 152 pruned. Trial was pruned at epoch 1.\n",
      "[I 2024-08-30 09:20:05,960] Trial 153 pruned. Trial was pruned at epoch 3.\n",
      "[I 2024-08-30 09:30:10,631] Trial 154 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 09:30:17,766] Trial 155 pruned. Trial was pruned at epoch 21.\n",
      "[I 2024-08-30 09:30:20,927] Trial 156 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 09:30:22,938] Trial 157 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 09:30:24,385] Trial 158 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 09:30:26,287] Trial 159 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 09:30:28,159] Trial 160 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 09:30:30,045] Trial 161 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 09:30:31,806] Trial 162 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 09:32:24,192] Trial 163 finished with value: 0.0 and parameters: {'n_layers': 5, 'n_units_l0': 32, 'dropout_l0': 0.45814777210129937, 'n_units_l1': 32, 'dropout_l1': 0.3623200759849024, 'n_units_l2': 35, 'dropout_l2': 0.529491645849016, 'n_units_l3': 14, 'dropout_l3': 0.340336942502351, 'n_units_l4': 32, 'dropout_l4': 0.4509074053651387, 'optimizer': 'rmsprop', 'learning_rate': 0.009137732188043143, 'batch_size': 160}. Best is trial 17 with value: 0.0.\n",
      "[I 2024-08-30 09:32:25,821] Trial 164 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 09:32:45,893] Trial 165 finished with value: 3.899551168085047e-11 and parameters: {'n_layers': 5, 'n_units_l0': 29, 'dropout_l0': 0.4490199713059429, 'n_units_l1': 32, 'dropout_l1': 0.38889670212653993, 'n_units_l2': 35, 'dropout_l2': 0.5303571430645662, 'n_units_l3': 14, 'dropout_l3': 0.3540566642514213, 'n_units_l4': 32, 'dropout_l4': 0.42347187121114277, 'optimizer': 'rmsprop', 'learning_rate': 0.009498663971166197, 'batch_size': 128}. Best is trial 17 with value: 0.0.\n",
      "[I 2024-08-30 09:32:53,648] Trial 166 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 09:33:13,215] Trial 167 finished with value: 0.0 and parameters: {'n_layers': 5, 'n_units_l0': 32, 'dropout_l0': 0.4007600796809774, 'n_units_l1': 26, 'dropout_l1': 0.36605823560796713, 'n_units_l2': 38, 'dropout_l2': 0.34316316573899414, 'n_units_l3': 11, 'dropout_l3': 0.34097046673228915, 'n_units_l4': 23, 'dropout_l4': 0.43173201491175456, 'optimizer': 'rmsprop', 'learning_rate': 0.007378481202001928, 'batch_size': 192}. Best is trial 17 with value: 0.0.\n",
      "[I 2024-08-30 09:33:32,741] Trial 168 finished with value: 1.5598204672340188e-10 and parameters: {'n_layers': 5, 'n_units_l0': 29, 'dropout_l0': 0.41093668929331145, 'n_units_l1': 29, 'dropout_l1': 0.4514918346267797, 'n_units_l2': 35, 'dropout_l2': 0.5364859997427299, 'n_units_l3': 38, 'dropout_l3': 0.3658852138951508, 'n_units_l4': 26, 'dropout_l4': 0.4812826012418664, 'optimizer': 'rmsprop', 'learning_rate': 0.009189710376589196, 'batch_size': 128}. Best is trial 17 with value: 0.0.\n",
      "[I 2024-08-30 09:33:38,119] Trial 169 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 09:33:45,585] Trial 170 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 09:33:50,900] Trial 171 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 09:33:56,807] Trial 172 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 09:34:02,815] Trial 173 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 09:34:08,727] Trial 174 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 09:34:15,126] Trial 175 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 09:34:32,911] Trial 176 finished with value: 0.0 and parameters: {'n_layers': 5, 'n_units_l0': 35, 'dropout_l0': 0.4180611292717241, 'n_units_l1': 26, 'dropout_l1': 0.3703942865839802, 'n_units_l2': 38, 'dropout_l2': 0.5121718427857048, 'n_units_l3': 11, 'dropout_l3': 0.41212803427250383, 'n_units_l4': 8, 'dropout_l4': 0.43201420705585375, 'optimizer': 'rmsprop', 'learning_rate': 0.00795733801640958, 'batch_size': 224}. Best is trial 17 with value: 0.0.\n",
      "[I 2024-08-30 09:34:38,987] Trial 177 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 09:35:12,513] Trial 178 finished with value: 0.0 and parameters: {'n_layers': 5, 'n_units_l0': 32, 'dropout_l0': 0.3693305410694564, 'n_units_l1': 20, 'dropout_l1': 0.3790494729938027, 'n_units_l2': 26, 'dropout_l2': 0.4744765736208269, 'n_units_l3': 14, 'dropout_l3': 0.31949571839280694, 'n_units_l4': 23, 'dropout_l4': 0.4418204151228725, 'optimizer': 'rmsprop', 'learning_rate': 0.006973956393210344, 'batch_size': 96}. Best is trial 17 with value: 0.0.\n",
      "[I 2024-08-30 09:35:40,784] Trial 179 finished with value: 0.0 and parameters: {'n_layers': 5, 'n_units_l0': 20, 'dropout_l0': 0.4071430559808759, 'n_units_l1': 32, 'dropout_l1': 0.3342719182928072, 'n_units_l2': 38, 'dropout_l2': 0.5014256160277243, 'n_units_l3': 11, 'dropout_l3': 0.3281699414324047, 'n_units_l4': 5, 'dropout_l4': 0.3884769403833635, 'optimizer': 'adam', 'learning_rate': 0.008702202362526104, 'batch_size': 128}. Best is trial 17 with value: 0.0.\n",
      "[I 2024-08-30 09:35:46,336] Trial 180 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 09:36:03,034] Trial 181 finished with value: 0.0 and parameters: {'n_layers': 5, 'n_units_l0': 35, 'dropout_l0': 0.418945226159693, 'n_units_l1': 26, 'dropout_l1': 0.3664588378142091, 'n_units_l2': 38, 'dropout_l2': 0.5286946113974308, 'n_units_l3': 11, 'dropout_l3': 0.4175774075643365, 'n_units_l4': 17, 'dropout_l4': 0.427922755217797, 'optimizer': 'rmsprop', 'learning_rate': 0.007878582984733035, 'batch_size': 224}. Best is trial 17 with value: 0.0.\n",
      "[I 2024-08-30 09:36:19,556] Trial 182 finished with value: 0.0 and parameters: {'n_layers': 5, 'n_units_l0': 35, 'dropout_l0': 0.42462741419696326, 'n_units_l1': 26, 'dropout_l1': 0.36147814309656595, 'n_units_l2': 38, 'dropout_l2': 0.510312826933641, 'n_units_l3': 14, 'dropout_l3': 0.4366835419176908, 'n_units_l4': 5, 'dropout_l4': 0.43392570812611514, 'optimizer': 'rmsprop', 'learning_rate': 0.006500698967911396, 'batch_size': 224}. Best is trial 17 with value: 0.0.\n",
      "[I 2024-08-30 09:36:25,222] Trial 183 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 09:36:31,161] Trial 184 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 09:36:37,284] Trial 185 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 09:36:42,996] Trial 186 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 09:36:49,871] Trial 187 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 09:36:56,009] Trial 188 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 09:37:03,910] Trial 189 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 09:37:09,490] Trial 190 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 09:37:21,150] Trial 191 pruned. Trial was pruned at epoch 1.\n",
      "[I 2024-08-30 09:37:26,892] Trial 192 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 09:37:33,169] Trial 193 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 09:37:41,180] Trial 194 pruned. Trial was pruned at epoch 3.\n",
      "[I 2024-08-30 09:37:47,908] Trial 195 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 09:37:52,279] Trial 196 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 09:38:03,254] Trial 197 pruned. Trial was pruned at epoch 20.\n",
      "[I 2024-08-30 09:38:07,440] Trial 198 pruned. Trial was pruned at epoch 0.\n",
      "[I 2024-08-30 09:38:12,293] Trial 199 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.5490 - loss: 0.4323\n",
      "Epoch 2/100\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9080 - loss: 0.1259\n",
      "Epoch 3/100\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9762 - loss: 0.0388\n",
      "Epoch 4/100\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9881 - loss: 0.0211\n",
      "Epoch 5/100\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9892 - loss: 0.0154\n",
      "Epoch 6/100\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9904 - loss: 0.0125\n",
      "Epoch 7/100\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9894 - loss: 0.0110\n",
      "Epoch 8/100\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9903 - loss: 0.0122\n",
      "Epoch 9/100\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9908 - loss: 0.0110\n",
      "Epoch 10/100\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9927 - loss: 0.0084\n",
      "Epoch 11/100\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9915 - loss: 0.0144\n",
      "Epoch 12/100\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9914 - loss: 0.0112\n",
      "Epoch 13/100\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9929 - loss: 0.0085\n",
      "Epoch 14/100\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9907 - loss: 0.0112\n",
      "Epoch 15/100\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9911 - loss: 0.0119\n",
      "Epoch 16/100\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9905 - loss: 0.0108\n",
      "Epoch 17/100\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9922 - loss: 0.0082\n",
      "Epoch 18/100\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9906 - loss: 0.0113\n",
      "Epoch 19/100\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9927 - loss: 0.0085\n",
      "Epoch 20/100\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9918 - loss: 0.0091\n",
      "Epoch 21/100\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9918 - loss: 0.0100\n",
      "Epoch 22/100\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9922 - loss: 0.0114\n",
      "Epoch 23/100\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9893 - loss: 0.0127\n",
      "Epoch 24/100\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9938 - loss: 0.0075\n",
      "Epoch 25/100\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9893 - loss: 0.0206\n",
      "Epoch 26/100\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9923 - loss: 0.0073\n",
      "Epoch 27/100\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9923 - loss: 0.0094\n",
      "Epoch 28/100\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9930 - loss: 0.0059\n",
      "Epoch 29/100\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9931 - loss: 0.0072\n",
      "Epoch 30/100\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9927 - loss: 0.0079\n",
      "Epoch 31/100\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9908 - loss: 0.0081\n",
      "Epoch 32/100\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9938 - loss: 0.0069\n",
      "Epoch 33/100\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9911 - loss: 0.0084\n",
      "Epoch 34/100\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9931 - loss: 0.0088\n",
      "Epoch 35/100\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9922 - loss: 0.0128\n",
      "Epoch 36/100\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9909 - loss: 0.0115\n",
      "Epoch 37/100\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9934 - loss: 0.0074\n",
      "Epoch 38/100\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9911 - loss: 0.0080\n",
      "Epoch 39/100\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9924 - loss: 0.0073\n",
      "Epoch 40/100\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9933 - loss: 0.0059\n",
      "Epoch 41/100\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9934 - loss: 0.0069\n",
      "Epoch 42/100\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9923 - loss: 0.0065\n",
      "Epoch 43/100\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9925 - loss: 0.0062\n",
      "Epoch 44/100\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9934 - loss: 0.0072\n",
      "Epoch 45/100\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9921 - loss: 0.0069\n",
      "Epoch 46/100\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9919 - loss: 0.0094\n",
      "Epoch 47/100\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9925 - loss: 0.0072\n",
      "Epoch 48/100\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9936 - loss: 0.0081\n",
      "Epoch 49/100\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9932 - loss: 0.0061\n",
      "Epoch 50/100\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9917 - loss: 0.0078\n",
      "Epoch 51/100\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9925 - loss: 0.0078\n",
      "Epoch 52/100\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9923 - loss: 0.0081\n",
      "Epoch 53/100\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9929 - loss: 0.0080\n",
      "Epoch 54/100\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9923 - loss: 0.0062\n",
      "Epoch 55/100\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9912 - loss: 0.0086\n",
      "Epoch 56/100\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9921 - loss: 0.0075\n",
      "Epoch 57/100\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9946 - loss: 0.0054\n",
      "Epoch 58/100\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9939 - loss: 0.0059\n",
      "Epoch 59/100\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9932 - loss: 0.0085\n",
      "Epoch 60/100\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9922 - loss: 0.0068\n",
      "Epoch 61/100\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9939 - loss: 0.0057\n",
      "Epoch 62/100\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9940 - loss: 0.0057\n",
      "Epoch 63/100\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9905 - loss: 0.0111\n",
      "Epoch 64/100\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9936 - loss: 0.0056\n",
      "Epoch 65/100\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9927 - loss: 0.0119\n",
      "Epoch 66/100\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9942 - loss: 0.0058\n",
      "Epoch 67/100\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9935 - loss: 0.0104\n",
      "Epoch 68/100\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9930 - loss: 0.0071\n",
      "Epoch 69/100\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9945 - loss: 0.0057\n",
      "Epoch 70/100\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9926 - loss: 0.0074\n",
      "Epoch 71/100\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9921 - loss: 0.0080\n",
      "Epoch 72/100\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9926 - loss: 0.0081\n",
      "Epoch 73/100\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9918 - loss: 0.0082\n",
      "Epoch 74/100\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9944 - loss: 0.0072\n",
      "Epoch 75/100\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9928 - loss: 0.0060\n",
      "Epoch 76/100\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9926 - loss: 0.0059\n",
      "Epoch 77/100\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9926 - loss: 0.0087\n",
      "Epoch 78/100\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9931 - loss: 0.0075\n",
      "Epoch 79/100\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9904 - loss: 0.0078\n",
      "Epoch 80/100\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9933 - loss: 0.0057\n",
      "Epoch 81/100\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9923 - loss: 0.0074\n",
      "Epoch 82/100\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9917 - loss: 0.0080\n",
      "Epoch 83/100\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9922 - loss: 0.0075\n",
      "Epoch 84/100\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9928 - loss: 0.0067\n",
      "Epoch 85/100\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9924 - loss: 0.0085\n",
      "Epoch 86/100\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9944 - loss: 0.0059\n",
      "Epoch 87/100\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9936 - loss: 0.0064\n",
      "Epoch 88/100\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9930 - loss: 0.0072\n",
      "Epoch 89/100\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9929 - loss: 0.0073\n",
      "Epoch 90/100\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9932 - loss: 0.0061\n",
      "Epoch 91/100\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9924 - loss: 0.0063\n",
      "Epoch 92/100\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9927 - loss: 0.0068\n",
      "Epoch 93/100\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9921 - loss: 0.0063\n",
      "Epoch 94/100\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9932 - loss: 0.0109\n",
      "Epoch 95/100\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9934 - loss: 0.0055\n",
      "Epoch 96/100\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9931 - loss: 0.0081\n",
      "Epoch 97/100\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9929 - loss: 0.0076\n",
      "Epoch 98/100\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9921 - loss: 0.0080\n",
      "Epoch 99/100\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9926 - loss: 0.0072\n",
      "Epoch 100/100\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9935 - loss: 0.0062\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "Confusion Matrix:\n",
      "[[528 141]\n",
      " [205 188]]\n",
      "Classification Report:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Green       0.72      0.79      0.75       669\n",
      "         Red       0.57      0.48      0.52       393\n",
      "\n",
      "    accuracy                           0.67      1062\n",
      "   macro avg       0.65      0.63      0.64      1062\n",
      "weighted avg       0.67      0.67      0.67      1062\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['label_encoder_v2.pkl']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from imblearn.combine import SMOTETomek\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import optuna\n",
    "from optuna.integration import TFKerasPruningCallback\n",
    "import tensorflow as tf\n",
    "import joblib\n",
    "\n",
    "# Load dataset\n",
    "train_data = pd.read_csv('Training_dt.csv')\n",
    "train_data = train_data.drop('Mask No', axis=1)\n",
    "test_data = pd.read_csv('Testing_dt.csv')\n",
    "test_data = test_data.drop('Mask No', axis=1)\n",
    "\n",
    "# Define columns\n",
    "numerical_cols = ['LEASE_TENOR_INCLUDING_HP', 'CUSTOMER AGE', 'Exp', 'YOM']\n",
    "categorical_cols = ['PRODUCT_NAME', 'Sub_purpose_code_based_on_risk', 'CRIB_SCORE', 'TOTAL INCOME',\n",
    "                    'Percentage_of_Total_Installments_to_Total_Current_Balance_slabs',\n",
    "                    'Percentage_of_Total_Current_Balance_to_Total_Amount_Granted_Limit_slabs',\n",
    "                    'Percentage_of_Total_Arrears_Amount_to_Total_Amount_Granted_Limit_slabs']\n",
    "target_col = 'Cluster'\n",
    "\n",
    "# Encode categorical columns\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))])\n",
    "\n",
    "# Combine preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)])\n",
    "\n",
    "# Preprocess the data\n",
    "X_train = train_data[numerical_cols + categorical_cols]\n",
    "y_train = train_data[target_col]\n",
    "\n",
    "X_test = test_data[numerical_cols + categorical_cols]\n",
    "y_test = test_data[target_col]\n",
    "\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "# Encode target variable\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# Handle imbalance using advanced methods\n",
    "def handle_imbalance(X, y):\n",
    "    imbalance_methods = {\n",
    "        'SMOTETomek': SMOTETomek(sampling_strategy='auto'),\n",
    "        'SMOTE': SMOTE(sampling_strategy='auto'),\n",
    "        'ADASYN': ADASYN(sampling_strategy='auto')\n",
    "    }\n",
    "\n",
    "    best_method = None\n",
    "    best_score = -np.inf\n",
    "    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    for name, method in imbalance_methods.items():\n",
    "        X_resampled, y_resampled = method.fit_resample(X, y)\n",
    "        clf = RandomForestClassifier()\n",
    "        scores = cross_val_score(clf, X_resampled, y_resampled, cv=kf, scoring='roc_auc_ovr')\n",
    "        mean_score = np.mean(scores)\n",
    "        if mean_score > best_score:\n",
    "            best_score = mean_score\n",
    "            best_method = method\n",
    "\n",
    "    return best_method.fit_resample(X, y)\n",
    "\n",
    "X_train_resampled, y_train_resampled = handle_imbalance(X_train_processed, y_train_encoded)\n",
    "\n",
    "# Add class weights to handle imbalance directly in the model\n",
    "class_weights = dict(enumerate(np.bincount(y_train_resampled) / len(y_train_resampled)))\n",
    "\n",
    "# Define ML algorithms\n",
    "ml_algorithms = {\n",
    "    'RandomForest': RandomForestClassifier(class_weight='balanced'),\n",
    "    'GradientBoosting': GradientBoostingClassifier(),\n",
    "    'LogisticRegression': LogisticRegression(max_iter=1000, class_weight='balanced')\n",
    "}\n",
    "\n",
    "# Cross-validation to select the best ML algorithm\n",
    "best_algorithm = None\n",
    "best_score = -np.inf\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for name, clf in ml_algorithms.items():\n",
    "    try:\n",
    "        scores = cross_val_score(clf, X_train_resampled, y_train_resampled, \n",
    "                                 cv=kf, \n",
    "                                 scoring='roc_auc_ovr')\n",
    "        mean_score = np.mean(scores)\n",
    "        if mean_score > best_score:\n",
    "            best_score = mean_score\n",
    "            best_algorithm = clf\n",
    "    except Exception as e:\n",
    "        print(f\"Error with {name}: {e}\")\n",
    "\n",
    "# Check if a valid algorithm was selected\n",
    "if best_algorithm is None:\n",
    "    raise ValueError(\"No valid machine learning algorithm was selected.\")\n",
    "\n",
    "# Fit the best ML algorithm on the entire dataset\n",
    "best_algorithm.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Use the predictions of the best ML algorithm as additional input to the ANN\n",
    "ml_train_predictions = best_algorithm.predict_proba(X_train_resampled)\n",
    "ml_test_predictions = best_algorithm.predict_proba(X_test_processed)\n",
    "\n",
    "# Concatenate the ML algorithm's predictions with the original input features\n",
    "X_train_combined = np.hstack([X_train_resampled, ml_train_predictions])\n",
    "X_test_combined = np.hstack([X_test_processed, ml_test_predictions])\n",
    "\n",
    "# Define the model using the functional API and Optuna for hyperparameter tuning\n",
    "def create_model(trial):\n",
    "    n_layers = trial.suggest_int('n_layers', 5, 7)\n",
    "    inputs = Input(shape=(X_train_combined.shape[1],))\n",
    "    x = inputs\n",
    "\n",
    "    for i in range(n_layers):\n",
    "        num_neurons = trial.suggest_int(f'n_units_l{i}', 2, 39, step=3)\n",
    "        x = Dense(num_neurons, activation='relu')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        dropout_rate = trial.suggest_float(f'dropout_l{i}', 0.2, 0.6)\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    outputs = Dense(len(np.unique(y_train_resampled)), activation='softmax')(x)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    optimizer_name = trial.suggest_categorical('optimizer', ['adam', 'rmsprop'])\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)\n",
    "    \n",
    "    if optimizer_name == 'adam':\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "    else:\n",
    "        optimizer = RMSprop(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    model = create_model(trial)\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "    \n",
    "    history = model.fit(X_train_combined, y_train_resampled, \n",
    "                        validation_split=0.2, \n",
    "                        epochs=100, \n",
    "                        batch_size=trial.suggest_int('batch_size', 32, 256, step=32),\n",
    "                        callbacks=[TFKerasPruningCallback(trial, 'val_loss'), early_stopping],\n",
    "                        class_weight=class_weights,\n",
    "                        verbose=0)\n",
    "    \n",
    "    val_loss = history.history['val_loss'][-1]\n",
    "    return val_loss\n",
    "\n",
    "# Run the hyperparameter optimization\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=200)  # Increase the number of trials\n",
    "\n",
    "# Train the best model on the entire dataset\n",
    "best_trial = study.best_trial\n",
    "model = create_model(best_trial)\n",
    "\n",
    "# Perform training on the full training data\n",
    "model.fit(X_train_combined, y_train_resampled, \n",
    "          epochs=100,  \n",
    "          batch_size=best_trial.params['batch_size'],\n",
    "          class_weight=class_weights,\n",
    "          verbose=1)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred_prob = model.predict(X_test_combined)\n",
    "\n",
    "# Adjust the decision threshold to reduce FP and FN\n",
    "threshold = 0.40  # Further adjust this value based on validation set\n",
    "y_pred = (y_pred_prob[:, 1] >= threshold).astype(int)\n",
    "y_pred = label_encoder.inverse_transform(y_pred)\n",
    "\n",
    "# Evaluate model performance\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Save the model and other necessary files\n",
    "model.save('best_ann_model_v2.h5')\n",
    "joblib.dump(best_algorithm, 'best_ml_model_v2.pkl')\n",
    "joblib.dump(preprocessor, 'preprocessor_v2.pkl')\n",
    "joblib.dump(label_encoder, 'label_encoder_v2.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Green       0.78      0.78      0.78       881\n",
      "         Red       0.46      0.48      0.47       361\n",
      "\n",
      "    accuracy                           0.69      1242\n",
      "   macro avg       0.62      0.63      0.62      1242\n",
      "weighted avg       0.69      0.69      0.69      1242\n",
      "\n",
      "Predictions for the additional test set saved as 'My_test_predictions_final.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the additional test dataset\n",
    "test_data_additional = pd.read_csv('My_Test_data.csv')\n",
    "# test_data_additional = test_data_additional.drop('Mask No', axis=1)\n",
    "\n",
    "# Define columns\n",
    "numerical_cols = ['LEASE_TENOR_INCLUDING_HP', 'CUSTOMER AGE', 'Exp', 'YOM']\n",
    "categorical_cols = ['PRODUCT_NAME', 'Sub_purpose_code_based_on_risk', 'CRIB_SCORE', 'TOTAL INCOME',\n",
    "                    'Percentage_of_Total_Installments_to_Total_Current_Balance_slabs',\n",
    "                    'Percentage_of_Total_Current_Balance_to_Total_Amount_Granted_Limit_slabs',\n",
    "                    'Percentage_of_Total_Arrears_Amount_to_Total_Amount_Granted_Limit_slabs']\n",
    "\n",
    "# Load the preprocessor\n",
    "preprocessor = joblib.load('preprocessor_v2.pkl')\n",
    "\n",
    "# Preprocess the additional test data\n",
    "X_test_additional_processed = preprocessor.transform(test_data_additional[numerical_cols + categorical_cols])\n",
    "\n",
    "# Load the best ML model\n",
    "best_algorithm = joblib.load('best_ml_model_v2.pkl')\n",
    "\n",
    "# Generate predictions using the ML model\n",
    "ml_test_predictions_additional = best_algorithm.predict_proba(X_test_additional_processed)\n",
    "\n",
    "# Concatenate the ML algorithm's predictions with the original input features\n",
    "X_test_additional_combined = np.hstack([X_test_additional_processed, ml_test_predictions_additional])\n",
    "\n",
    "# Load the trained ANN model\n",
    "model = load_model('best_ann_model_v2.h5')\n",
    "\n",
    "# Generate predictions for the additional test set\n",
    "y_pred_prob_additional = model.predict(X_test_additional_combined)\n",
    "y_pred_additional = np.argmax(y_pred_prob_additional, axis=1)\n",
    "\n",
    "# Load the LabelEncoder to decode the target variable\n",
    "label_encoder = joblib.load('label_encoder_v2.pkl')\n",
    "\n",
    "# Inverse transform the encoded predictions to get the original labels\n",
    "y_pred_additional_labels = label_encoder.inverse_transform(y_pred_additional)\n",
    "\n",
    "# Save the predictions to a new CSV file\n",
    "test_data_additional['Predicted_Cluster'] = y_pred_additional_labels\n",
    "test_data_additional.to_csv('My_test_predictions_final.csv', index=False)\n",
    "\n",
    "# Generate the confusion matrix\n",
    "y_true = test_data_additional['Original Cluster']  # Replace 'Original Cluster' with the correct column name\n",
    "y_pred = test_data_additional['Predicted_Cluster']\n",
    "\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Visualize the confusion matrix using seaborn\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "# Optionally, print the classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred))\n",
    "\n",
    "print(\"Predictions for the additional test set saved as 'My_test_predictions_final.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
